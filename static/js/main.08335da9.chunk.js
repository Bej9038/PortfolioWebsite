(this.webpackJsonpportfolio=this.webpackJsonpportfolio||[]).push([[0],{37:function(e,t,a){},38:function(e,t,a){},39:function(e,t,a){},40:function(e,t,a){},41:function(e,t,a){},42:function(e,t,a){},43:function(e,t,a){},44:function(e,t,a){},45:function(e,t,a){},53:function(e,t,a){"use strict";a.r(t);var s=a(1),i=a.n(s),n=a(25),o=a.n(n),r=a(10),c=(a(37),a(38),a(39),a(40),a(41),a(42),a(43),a(44),a(45),a.p,a(8)),l=a(12),d=a(5),h=a(0);var m=function(){return Object(h.jsx)("div",{className:"NavBar",children:Object(h.jsxs)("nav",{className:"nav",children:[Object(h.jsxs)("header",{className:"hero",children:[Object(h.jsx)("h1",{className:"hero-text",children:Object(h.jsx)("span",{children:" Ben Jordan"})}),Object(h.jsx)("p",{className:"home-sub-text",children:"Machine Learning & Software Engineering"}),Object(h.jsxs)("div",{className:"icons",children:[Object(h.jsx)("a",{target:"_blank",href:"https://github.com/Bej9038",className:"icon-link",children:Object(h.jsx)(l.a,{icon:d.b,className:"icon"})}),Object(h.jsx)("a",{target:"_blank",href:"https://www.linkedin.com/in/ben-jordan-b745a0194/",className:"icon-link",children:Object(h.jsx)(l.a,{icon:d.c,className:"icon"})})]})]}),Object(h.jsxs)("ul",{className:"nav-items",children:[Object(h.jsx)("lin",{className:"nav-item",children:Object(h.jsx)(c.b,{to:"/",exact:!0,activeClassName:"active",children:"Home"})}),Object(h.jsx)("lin",{className:"nav-item",children:Object(h.jsx)(c.b,{to:"/Projects",exact:!0,activeClassName:"active",children:"Projects"})}),Object(h.jsx)("lin",{className:"nav-item",children:Object(h.jsx)(c.b,{to:"/Contact",exact:!0,activeClassName:"active",children:"Contact"})})]}),Object(h.jsx)("div",{className:"footer footer",children:Object(h.jsx)("p",{children:"@2024 Ben Jordan"})})]})})};var u=function(e){var t=e.title,a=e.progress,s="title25 .9s ease forwards";return 50===a?s="title50 .9s ease forwards":75===a?s="title75 .9s ease forwards":100===a&&(s="title100 .9s ease forwards"),Object(h.jsxs)("div",{className:"Title",children:[Object(h.jsx)("div",{className:"page-bar",children:Object(h.jsx)("div",{className:"page-bar-progress",style:{animation:s}})}),Object(h.jsxs)("h3",{children:[t,Object(h.jsx)("span",{children:t})]})]})};var p=function(e){var t=e.skill;return e.progress,Object(h.jsx)("div",{className:"Skills",children:Object(h.jsx)("div",{className:"skills-container",children:Object(h.jsx)("h5",{className:"skill-title",children:t})})})},g=a.p+"static/media/Ben Jordan [May 2024].d99ca556.pdf";var b=function(){return Object(h.jsx)("div",{className:"ImageSection",children:Object(h.jsxs)("div",{className:"about-info",children:[Object(h.jsxs)("p",{className:"about-text",children:[Object(h.jsx)("span",{children:"I am a recent graduate of cornell university working in industry as a machine learning engineer."}),Object(h.jsx)("br",{}),Object(h.jsx)("br",{}),"I enjoy learning about applied math, machine learning, and computer systems. I also just really like working on software projects in general.",Object(h.jsx)("br",{}),Object(h.jsx)("br",{}),"Outside of work, I like being active, visiting new places, and music. As an undergrad, I was a member of RIT's varsity track and field team."]}),Object(h.jsx)("a",{style:{color:"var(--font-color2)"},href:g,download:!0,children:Object(h.jsx)("button",{className:"btn",children:"Download CV"})})]})})};var f=function(){return Object(h.jsxs)("div",{className:"AboutPage",children:[Object(h.jsx)(u,{title:"About Me",progress:25}),Object(h.jsx)(b,{}),Object(h.jsx)(u,{title:"Skills",progress:50}),Object(h.jsxs)("div",{className:"skills-group",children:[Object(h.jsx)(p,{skill:"Python",progress:"100%"}),Object(h.jsx)(p,{skill:"PyTorch",progress:"100%"}),Object(h.jsx)(p,{skill:"NumPy",progress:"100%"}),Object(h.jsx)(p,{skill:"C++",progress:"100%"}),Object(h.jsx)(p,{skill:"C",progress:"100%"}),Object(h.jsx)(p,{skill:"CUDA",progress:"100%"}),Object(h.jsx)(p,{skill:"Docker",progress:"100%"}),Object(h.jsx)(p,{skill:"Git",progress:"100%"}),Object(h.jsx)(p,{skill:"Linux",progress:"100%"}),Object(h.jsx)(p,{skill:"LaTeX",progress:"100%"})]})]})},j=a(3),w=a(31);var v=a(22),x=a.n(v);var y=function(e){var t=e.menuItem,a=e.id,i=e.setId,n=Object(s.useState)(!1),o=Object(r.a)(n,2),c=o[0],d=o[1];return Object(h.jsx)("div",{className:"MenuItem",children:t.map((function(e){return Object(h.jsxs)("div",{className:"portfolio",children:[Object(h.jsxs)("h5",{children:[e.link?Object(h.jsx)("a",{target:"_blank",href:e.link,children:Object(h.jsx)(l.a,{icon:e.icon,className:"icon"})}):"",Object(h.jsx)("div",{className:"title",children:e.title})]}),Object(h.jsxs)("div",{style:{maxHeight:"".concat(!c||-1!==a&&e.id!==a?"24vh":"400vh")},className:"project-content",children:[Object(h.jsx)("div",{className:"".concat(!c||-1!==a&&e.id!==a?"text-cover":"text-cover-plain")}),Object(h.jsx)("button",{className:"expand-btn",onClick:function(){c?(i(-1),d(!1)):(i(e.id),d(!0))},children:"Expand"}),Object(h.jsxs)("div",{className:"flex-container",children:[Object(h.jsx)("p",{className:"description",children:e.descriptions&&e.descriptions[0]?e.descriptions[0]:""}),function(){if(e.images&&e.images[0])return Object(h.jsxs)("div",{children:[Object(h.jsx)("img",{className:"img",src:e.images[0],alt:""}),Object(h.jsx)("div",{className:"img-caption",children:e.imagecap[0]})]})}(),Object(h.jsx)("p",{className:"description",children:e.descriptions&&e.descriptions[1]?e.descriptions[1]:""}),function(){if(e.images&&e.images[1])return Object(h.jsxs)("div",{children:[Object(h.jsx)("img",{className:"img",src:e.images[1],alt:""}),Object(h.jsx)("div",{className:"img-caption",children:e.imagecap[1]})]})}(),function(){if(e.audio&&e.audiocap)return Object(h.jsxs)("div",{className:"audio-container",children:[Object(h.jsxs)("div",{children:[Object(h.jsx)(x.a,{className:"audio-player",src:e.audio[0],controls:!0}),Object(h.jsx)("div",{className:"audio-caption",children:e.audiocap[0]})]}),Object(h.jsxs)("div",{children:[Object(h.jsx)(x.a,{className:"audio-player",src:e.audio[1],controls:!0}),Object(h.jsx)("div",{className:"audio-caption",children:e.audiocap[1]})]})]})}(),Object(h.jsx)("p",{className:"description",children:e.descriptions&&e.descriptions[2]?e.descriptions[2]:""}),function(){if(e.images&&e.images[2])return Object(h.jsxs)("div",{children:[Object(h.jsx)("img",{className:"img",src:e.images[2],alt:""}),Object(h.jsx)("div",{className:"img-caption",children:e.imagecap[2]})]})}(),Object(h.jsx)("p",{className:"description",children:e.descriptions&&e.descriptions[3]?e.descriptions[3]:""}),function(){if(e.images&&e.images[3])return Object(h.jsxs)("div",{children:[Object(h.jsx)("img",{className:"img",src:e.images[3],alt:""}),Object(h.jsx)("div",{className:"img-caption",children:e.imagecap[3]})]})}(),Object(h.jsx)("p",{className:"description",children:e.descriptions&&e.descriptions[4]?e.descriptions[4]:""})]})]})]},e.id)}))})},O=a.p+"static/media/dslabs1.d3e62336.PNG",N=a.p+"static/media/dslabs2.37fb7c6f.png",k=(a.p,a.p,a.p+"static/media/SAE1.ee662d30.PNG"),I=a.p+"static/media/SAE2.2881e408.PNG",T=a.p+"static/media/SAE4.5677207e.PNG",A=(a.p,a.p,a.p,a.p,a.p,a.p,a.p+"static/media/EQ2.4f946a9f.PNG"),P=(a.p,a.p,a.p,a.p+"static/media/FV3.8dd5368a.PNG"),S=a.p+"static/media/layer2_feat2.d8481bae.png",C=a.p+"static/media/FV5.f9dd787d.png",q=(a.p,a.p,a.p,a.p+"static/media/eq-dry.d1d955e3.mp3"),G=a.p+"static/media/eq-wet.9ed82bf8.mp3",E=a.p+"static/media/audiocraft1.7119c8f1.png",z=a.p+"static/media/frontend.c16b925e.PNG",M={synthlm1:"I believe that some form of natural langauge interface is the future of audio creation. After envisioning a natural language synthesizer, I knew I had the chance to apply my skill set and create something novel. Although there's been some work in the text to speech, text to music, and text to synth-parameter domains, I don't feel like any of these tools are ideal for music creators in their current state.",synthlm2:"Luckily, Meta AI has done some great research on audio generation. They released 3 models (MusicGen, AudioGen, and Encodec), along with a paper for each and an open-source codebase. MusicGen and AudioGen are relatively simple autoregressive transformer models that aim to predict time-steps in a discretized audio sequence. My goal is to retrain MusicGen to produce instrument samples rather than entire songs, which should be an easier task to learn, and hopefully more useful for music creators.",synthlm3:"For my dataset, I accumulated 160 GB of music production sample packs. For the text descriptions, about half of the packs already contain natural language descriptions. For the other half, I parsed the file paths of the samples to create multi-label descriptions. This works out quite well because sample pack file paths are very descriptive, and often times contain BPM or key signatures. (For example, one of the file paths looks like dataset\\Sounds of KSHMR Vol 4\\Drum Loops\\Drum Loops - By Genre\\Hip Hop Loops\\Old School\\KSHMR Hip Hop Old School 01 - 80BPM - Full.wav)",synthlm4:"As for training, I started by creating custom training code based off of the original codebase, and tested things on my local machine with the small 300M parameter model. I then trained the medium 1.1B model on a single A100  using a small dataset (~3 sample packs) just to be sure this task was learnable. I am currently training the large 3.3B parameter model on the 160GB dataset using 3 A6000s. It is quite interesting seeing how the learning rate and other hyperparameters affect the model's output. For example, I found in my testing that a lower learning rate almost directly corresponds to more detailed audio, but it is really easy to go too low and pick up noisy patterns!",synthlm5:"I am currently in the process of designing a frontend, and have deployed the model using serverless computing. I also have many planned improvements for the model's architecture and training process that I am incredibly excited about. It feels like there's a world of possibilities for this technology as evidenced by the progress made with language and images.",dslabs1:"DSLabs was a semester long project that I completed while taking Cornell's CS5414 (Distributed Computing) with Lorenzo Alvisi. This class was easily one of the most challenging, rewarding, and well designed courses I have ever taken. The project itself consisted of a framework that allows students to create and test distributed systems, along with four major labs where we were tasked with implementing a system similar in functionality to Google's Spanner.",dslabs2:'Lab 1 involved implementing an "at-most-once" key-value store (duplicate commands will only execute once, results are cached), along with a basic client and server. Lab 2 consisted of adding primary-backup replication to lab 1 using an all-knowing view service server that decides on primary backup configurations. This allows for state replication and consistency, but it also leaves us with a single point of failure (the view service).',dslabs3:"Lab 3 fixes this problem using Paxos. Paxos is an intriguing algorithm that allows a group of servers to be fault tolerant as long as a majority of servers in the group don't fail. It also guarantees that consensus can be reached during periods of synchrony. This part involved a bit too many hours of grinding in order to debug the system, but I'm proud to say that we passed all of the test cases.",dslabs4:"Lab 4 added on multi-key transactions and sharding. This allows the system to process operations in parallel thus increasing performance proportional to the number of server groups. We also had to implement two phase commit for agreement between server groups during transactions. On its own 2PC is susceptible to failures, but when paired with paxos provides agreement without any major availability issues.",sae1:"After working for Professor Kim in 2020, I was rehired as a part-time developer during my last semester of undergrad. During this semester, my task was to write a program that would allow Dr. Kim to collect data on how listeners interpret the spatial characteristics of audio. This was quite an enjoyable project because I was given a general overview of what to build, but every aspect of design and implementation was left up to me.",sae2:"The main feature of this program is an interactive 3D visual that is supposed to represent audio and its spacial characteristics. Users hear a series of songs, and then adjust a set of four attribute sliders (width, depth, immersion, and clarity) to morph the 3D visual until it best describes each song.",sae3:"Given that this project was a website, I decided to use Angular to get some more experience with it. I really like Angular + Typescript because of the OOP style, and because it helps me keep my projects organized. As for the 3D visual, I used a library called three.js. This was easily the most challenging part of the project since it was my first time doing anything related to graphics, but I learned a lot! I created a custom shader that morphs based on the music's volume and attribute slider values. ",sae4:"For the design of the software, I added UI components such as the attribute sliders and menus, a view component to hold the three.js canvas, a singular audio service that controls everything sound related and can be injected into any component, and a singular session values service used to maintain important values during each instance of the program (such as the current round number or username). This service was injected into the slider components and the view component, so both the sliders and the 3D graphic could have realtime access to the attribute sliders' values.",ipt1:"This was one of two projects created during my internship with Professor Sungyoung Kim at RIT. In collaboration with a team from University of Iowa, we attempted to evaluate the effectiveness of hearing devices called hybrid cochlear implants. The team was also interested in people's ability to understand speech depending on background noise level. I was given the task of independently creating a website that would allow the researchers to test participants hearing abilities and collect data. This ended up being a great opportunity because it was the largest project I had ever worked on, I got to learn about web development, and the team successfully published research using the data collected with the software. It's a good feeling when the software you make is used for something important.",ipt2:"The website contains 5 unique testing modules. The Inharmonicity Training and Speech-In-Noise tests have 2 and 3 different modes respectively. All tests heavily rely on the Javascript Web Audio API to generate and process sound at various frequencies and levels. The website is connected to a backend SQL database to store data for each user.",ipt3:'The main focus of the program was the Inharmonicity Training. In this module, users are presented with a box that produces a tone as their mouse hovers over it. The tone changes depending on the location of the mouse within the box. This tone consists of a group of low frequency oscillators and a group of high frequency oscillators set with precise frequency ratios. The goal is for the user to move their mouse inside the box until the tone sounds most "harmonic". After selecting a point, a gradient appears on the box that shows the user how correct their guess was. This gradient is calculated based on final mouse position and the randomized frequency ranges of the box\'s x and y axes set before each trial.',ipt4:"As part of a separate research question, I also added a speech-in-noise test that played a series of words alongside background noise. The user just has to guess which word was spoken.",gnn1:"For the final research project in my machine learning course at RIT, we were tasked with researching a deep learning architecture that we didn't cover in class, proposing an experiment, and then putting together a final paper and presentation. After doing some searching online, I decided to do my project on graph neural networks (specifically the GraphSAGE architecture).",gnn2:"The GraphSAGE algorithm works by first randomly sampling neighboring nodes of a given node in a graph, and then combining the original and sampled node's embeddings using an aggregation function. It's best to think of this as a generalization of convolution for graph data. Its also similar to the idea of summarizing word embeddings into sentence embeddings in NLP.",gnn3:"For the experiment portion of the project, I proposed that if we methodically select which aggregation function is used for each layer, then we will get better results out of our model (most GNNs use a single type of aggregation function for all layers). This is because some aggregators, such as mean pooling, may be better at summarizing earlier layers of embeddings than max pooling for example. To test this idea out, I used the CORA dataset (basically MNIST digits for GNNs) along with a PyTorch implementation of GraphSAGE. I implemented max and mean aggregators to go along with model.",gnn4:"The results showed that the proposed combination of aggregation layers (mean in earlier layers, and max in later layers) did in fact result in an increase in F1 score. However, this experiment was at such a small scale that testing on a larger dataset and model would need to be done to make any conclusions.",gnn5:"Overall, this turned out to be a great choice for my project. I was in the process of finishing up a graph theory course, so graphs concepts were fresh in my mind. It was also cool to study GNNs given that they aren't discussed nearly as much as other modalities of deep learning. Lastly, it was a good experience to learn a little more about writing a project proposal and creating an experiment. Although it was challenging doing a research project on a new topic in a little over a month, I'm glad I went through it.",eq1:"As an avid user of music production software, I always wondered how the tools I was using were created. I also wanted to brush up on my C++ skills. Therefore, to kill two birds with one stone I decided to dive in and make a parametric EQ (a common audio mixing effect).",eq2:"At first, I looked into writing VST3s (the standard format of an audio plugin) from scratch. However, it became apparent that this wasn't a simple feat for someone starting out. I ended up using an excellent C++ framework called JUCE that includes libraries for both audio processing and UI elements.",eq3:"My final EQ consisted of 2 notch filters for middle frequencies, a low pass filter to cut high frequencies, and a high pass filter to cut low frequencies. Each filter has a small bypass button above it. Writing the program involved routing audio input through the four filters, and connecting the UI knobs to the filter parameters. I also created a real-time frequency analyzer that sits behind a visualization of the EQ curve (the orange line). Lastly, I edited the algorithm for one of the built in JUCE filters to allow the user to select different slope values for the high and low pass filters.",eq4:"Although its not the worlds flashiest audio plugin, I am very satisfied that I got it working inside my own music production software. Now that I have more experience behind my belt, I can hopefully tackle more interesting ideas that I have.",omni1:"Alongside the cochlear implant testing software I created while interning with Professor Kim, I was also given the task of fleshing out another VR/AR project idea in my free time. This project was created in an attempt to preserve the aural characteristics of historical buildings.",omni2:"The software is designed to take in a dry audio sample (no reverb), and play it back as if it was inside a room, from any position in the room. The final result ended up working surprisingly well! The way something like this is done is by recording impulses, which are essentially isolated reverb tails, inside a room using an ambisonic 4 channel microphone. Then you convolve the source file with those impulse recordings, and combine the resulting 4 audio files in such a way such that they create B format ambisonic files.",omni3:"These B format files can then be converted to stereo using a javascript library called Omnitone. In the program, users can rotate their position in the room using the VR viewer. Users can also select different sound source and listening locations, and change which microphone is being used with the control panel on the bottom.",fv1:"This is a small program that came about as I was working on a project for my internship. I was looking into how interpretable one could make CNNs, and I came across a Stanford YouTube lecture on visualization for computer vision. One of the topics briefly discussed in the lecture was called gradient ascent, which is originally from the paper 'Understanding Neural Networks Through Deep Visualization'.",fv2:"A more common idea that is similar is saliency maps. Saliency maps are created by inputting an image into a network, and visualizing which pixels are most responsible for the prediction made. On the other hand, Gradient ascent aims to generate a new image that maximally activates a given output feature or class. The paper mentioned above discusses this technique, and also talks about how to make the results of gradient ascent into more interpretable, natural looking images.",fv3:"I tried implementing this algorithm myself using PyTorch, and after a little bit of fooling around with hyperparameters, I was able to get some cool images! I did it by taking a pre-trained PyTorch ResNet50 (trained on ImageNet1K), creating a blank image and passing it to the optimizer, and then performing gradient ascent on the image pixels until the image exactly maximized a specified class in the model. Following the advice of the paper, I also added gaussian blurring, L2 regularization, and gradient clipping which helped make the images much cleaner once I found the right settings. I also made it so the images could be created in parallel/batches. This way, one could potentially generate separate images for all output features in a given layer in one run.",fv4:"I ended up seeing some very interesting things during my testing. First, this technique does not appear to work with transformers. The resulting images look like a bunch of small squares with random patterns stitched together. This is presumably due to the way images are chopped up before being fed into a transformer. I also found it to be really cool how the visualizations that showed up in my images had different locations, shapes, and sizes depending on hyperparameters and image initialization. However, they still maintained the general characteristics of the feature/class that they represented. I think this is a great example of how CNNs can pick out specific features of an image regardless of location or other factors like size.",portfolio1:"This Website! During quarantine I wanted to give React a try, so I started with a tutorial I found online and made some edits. I recently did a re-design of the projects page as well.",temp:"Content coming soon..."},B=[{id:1,category:["2023"],link:"https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/",icon:d.a,descriptions:[M.synthlm1,M.synthlm2,M.synthlm3,M.synthlm4,M.synthlm5],title:"Entropy - Generative Audio Synthesizer",images:[E,z],imagecap:["1. The MusicGen Architecture","2. My UI Prototype"]},{id:2,category:["2023"],link:"https://github.com/emichael/dslabs",icon:d.b,descriptions:[M.dslabs1,M.dslabs2,M.dslabs3,M.dslabs4],images:[O,N],imagecap:["1: The Part-Time Parliament","2: The greatest moment of my life"],title:"DSLabs - Sharded Key-Value Store"},{id:10,category:["2023"],descriptions:[M.fv1,M.fv2,M.fv3,M.fv4,M.fv5],title:"FeatureViz - Deep Learning Visualization",link:"https://github.com/Bej9038/FeatureViz",icon:d.b,images:[P,S,C],imagecap:["1: A cool looking feature from the third group of layers in ResNet50","2: A more basic feature from an earlier layer","3: A faint visualization of the stingray class from the final classifier"]},{id:7,category:["2021"],link:"https://github.com/Bej9038/EQ",icon:d.b,descriptions:[M.eq1,M.eq2,M.eq3,M.eq4],audio:[q,G],audiocap:["A drum loop with the EQ bypassed","The same drum loop after applying the EQ curve shown in the image above"],images:[A],imagecap:["1: The EQ in action in my music production software"],title:"EQ Audio Plugin"},{id:3,category:["2022"],link:"https://github.com/Bej9038/SpatialAttributeEvaluation",icon:d.b,descriptions:[M.sae1,M.sae2,M.sae3,M.sae4],images:[T,k,I],imagecap:["1: Part of the initial project specifications","2: Welcome menu","3: Playing around with the controls"],title:"Spatial Attributes Evaluation"}];var F=[];B.map((function(e){return e.category.map((function(e){return F.push(e)}))}));var L=["All"].concat(Object(w.a)(F.filter((function(e,t,a){return a.indexOf(e)===t}))));var D=function(){var e=Object(s.useState)(L),t=Object(r.a)(e,2),a=(t[0],t[1],Object(s.useState)(B)),i=Object(r.a)(a,2),n=i[0],o=(i[1],Object(s.useState)(-1)),c=Object(r.a)(o,2),l=c[0],d=c[1];return Object(h.jsxs)("div",{className:"ProjectsPage",children:[Object(h.jsx)("div",{className:"title",children:Object(h.jsx)(u,{title:"Projects",progress:75})}),Object(h.jsx)("div",{className:"portfolio-menu",children:Object(h.jsx)(y,{menuItem:n,id:l,setId:d})})]})},R=a(32);var V=function(){var e=Object(R.a)("mqkwpwoa"),t=Object(r.a)(e,2),a=t[0],s=t[1];return a.succeeded&&(document.getElementsByClassName("success")[0].style.display="flex",document.getElementById("contact-form").reset()),Object(h.jsxs)("div",{className:"ContactPage",children:[Object(h.jsx)("div",{className:"contact-title",children:Object(h.jsx)(u,{title:"Contact",progress:100})}),Object(h.jsxs)("div",{className:"sections",children:[Object(h.jsx)("div",{className:"map-sect",children:Object(h.jsx)("iframe",{src:"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d47108.74706434036!2d-76.49837495!3d42.44270305!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89d08182e0af88f7%3A0xae52768a56ece74!2sIthaca%2C%20NY!5e0!3m2!1sen!2sus!4v1686931967031!5m2!1sen!2sus",width:"600",height:"450",style:{border:0},allowFullScreen:"",loading:"lazy",referrerPolicy:"no-referrer-when-downgrade"})}),Object(h.jsxs)("div",{className:"contact-sect",children:[Object(h.jsxs)("form",{autoComplete:"off",onSubmit:s,action:"https://formspree.io/f/mqkwpwoa",method:"post",id:"contact-form",children:[Object(h.jsxs)("div",{className:"form-group",children:[Object(h.jsx)("label",{htmlFor:"name",className:"label",children:"Name"}),Object(h.jsx)("input",{autoComplete:"new-password",type:"text",id:"name",className:"textio",name:"name"})]}),Object(h.jsx)("input",{type:"hidden",value:"prayer"}),Object(h.jsxs)("div",{className:"form-group",children:[Object(h.jsx)("label",{htmlFor:"email",className:"label",children:"Email"}),Object(h.jsx)("input",{autoComplete:"new-password",type:"text",id:"email",className:"textio",name:"email"})]}),Object(h.jsxs)("div",{className:"form-group",children:[Object(h.jsx)("label",{htmlFor:"message",className:"label",children:"Message"}),Object(h.jsx)("textarea",{id:"message",className:"textio",name:"message"})]}),Object(h.jsx)("button",{className:"email-btn",type:"submit",children:"Send"})]}),Object(h.jsx)("div",{className:"success",children:"Massage Received!"})]})]})]})},H=a(11);function U(){var e=Object(j.f)().pathname;return Object(s.useEffect)((function(){window.scrollTo(0,0)}),[e]),null}var Q=function(){var e=Object(s.useState)(!1),t=Object(r.a)(e,2),a=t[0];return t[1],Object(h.jsxs)("div",{className:"App",children:[Object(h.jsx)("div",{className:"nav-",children:Object(h.jsxs)(j.c,{children:[Object(h.jsx)(j.a,{path:"/",exact:!0,children:Object(h.jsxs)(c.b,{className:"rightarrow",to:"/Projects",exact:!0,children:[Object(h.jsx)(H.b,{}),Object(h.jsx)(H.b,{className:"rightarrow2"})]})}),Object(h.jsxs)(j.a,{path:"/Projects",exact:!0,children:[Object(h.jsxs)(c.b,{className:"leftarrow",to:"/",exact:!0,children:[Object(h.jsx)(H.a,{}),Object(h.jsx)(H.a,{className:"leftarrow2"})]}),Object(h.jsxs)(c.b,{className:"rightarrow",to:"/Contact",exact:!0,children:[Object(h.jsx)(H.b,{}),Object(h.jsx)(H.b,{className:"rightarrow2"})]})]}),Object(h.jsx)(j.a,{path:"/Contact",exact:!0,children:Object(h.jsxs)(c.b,{className:"leftarrow",to:"/Projects",exact:!0,children:[Object(h.jsx)(H.a,{}),Object(h.jsx)(H.a,{className:"leftarrow2"})]})})]})}),Object(h.jsx)("div",{className:"sidebar ".concat(a?"nav-toggle":""),children:Object(h.jsx)(m,{})}),Object(h.jsx)("div",{className:"main-content",children:Object(h.jsx)("div",{className:"content",children:Object(h.jsxs)(j.c,{children:[Object(h.jsxs)(j.a,{path:"/",exact:!0,children:[Object(h.jsx)(U,{}),Object(h.jsx)(f,{})]}),Object(h.jsxs)(j.a,{path:"/Projects",exact:!0,children:[Object(h.jsx)(U,{}),Object(h.jsx)(D,{})]}),Object(h.jsxs)(j.a,{path:"/Contact",exact:!0,children:[Object(h.jsx)(U,{}),Object(h.jsx)(V,{})]})]})})})]})};o.a.render(Object(h.jsx)(i.a.StrictMode,{children:Object(h.jsx)(c.a,{children:Object(h.jsx)(Q,{})})}),document.getElementById("root"))}},[[53,1,2]]]);
//# sourceMappingURL=main.08335da9.chunk.js.map