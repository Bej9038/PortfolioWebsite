(this.webpackJsonpportfolio=this.webpackJsonpportfolio||[]).push([[0],{36:function(e,t,a){},37:function(e,t,a){},38:function(e,t,a){},39:function(e,t,a){},40:function(e,t,a){},41:function(e,t,a){},42:function(e,t,a){},43:function(e,t,a){},44:function(e,t,a){},53:function(e,t,a){"use strict";a.r(t);var s=a(1),n=a.n(s),i=a(24),o=a.n(i),r=a(10),c=(a(36),a(37),a(38),a(39),a(40),a(41),a(42),a(43),a(44),a.p,a(8)),l=a(12),d=a(7),m=a(0);var h=function(){return Object(m.jsx)("div",{className:"NavBar",children:Object(m.jsxs)("nav",{className:"nav",children:[Object(m.jsx)("header",{className:"hero",children:Object(m.jsxs)("div",{className:"icons",children:[Object(m.jsx)("a",{target:"_blank",href:"https://github.com/EntropyAudio",className:"icon-link",children:Object(m.jsx)(l.a,{icon:d.b,className:"icon"})}),Object(m.jsx)("a",{target:"_blank",href:"https://www.linkedin.com/in/ben-jordan-b745a0194/",className:"icon-link",children:Object(m.jsx)(l.a,{icon:d.c,className:"icon"})})]})}),Object(m.jsxs)("ul",{className:"nav-items",children:[Object(m.jsx)("lin",{className:"nav-item",children:Object(m.jsx)(c.b,{to:"/",exact:!0,activeClassName:"active",children:"Home"})}),Object(m.jsx)("lin",{className:"nav-item",children:Object(m.jsx)(c.b,{to:"/Projects",exact:!0,activeClassName:"active",children:"Blog"})}),Object(m.jsx)("lin",{className:"nav-item",children:Object(m.jsx)(c.b,{to:"/Contact",exact:!0,activeClassName:"active",children:"Contact"})})]}),Object(m.jsx)("div",{className:"footer footer",children:Object(m.jsx)("p",{children:"@2025 Ben Jordan"})})]})})};var p=function(e){var t=e.title,a=e.progress,s="title25 .9s ease forwards";return 50===a?s="title50 .9s ease forwards":75===a?s="title75 .9s ease forwards":100===a&&(s="title100 .9s ease forwards"),Object(m.jsxs)("div",{className:"Title",children:[Object(m.jsx)("div",{className:"page-bar",children:Object(m.jsx)("div",{className:"page-bar-progress",style:{animation:s}})}),Object(m.jsxs)("h3",{children:[t,Object(m.jsx)("span",{children:t})]})]})};var u=a.p+"static/media/Benjamin_Jordan_Resume_2025.d1411ac7.pdf",b=a.p+"static/media/IMG_0426.2b35c307.JPEG";var j=function(){return Object(m.jsx)("div",{className:"ImageSection",children:Object(m.jsxs)("div",{className:"image-content",children:[Object(m.jsx)("div",{className:"about-info",children:Object(m.jsxs)("p",{className:"about-text",children:[Object(m.jsx)("span",{children:"I am a software engineer at Amazon and a graduate of Cornell & RIT CS."}),Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"In my current job, I primarily work on large scale spark ETL jobs and ML ops. I have a background in ML from my masters as well as from my previous role and internship.",Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"Outside of work I like to spend my time on my project, Entropy Audio, which aims to redefine how we create sound for music composition. I also love trying new food, cool architecture, and keeping up with cool research in CS."]})}),Object(m.jsx)("div",{className:"about-img-wrapper",children:Object(m.jsx)("img",{src:b,alt:"Benjamin Jordan headshot",className:"about-img"})})]})})};var g=function(){return Object(m.jsxs)("div",{className:"AboutPage",children:[Object(m.jsx)(p,{title:"Ben Jordan",progress:25}),Object(m.jsx)(j,{}),Object(m.jsx)(p,{title:"CV",progress:50}),Object(m.jsx)("a",{className:"download-link",style:{color:"var(--font-color2)"},href:u,download:!0,children:Object(m.jsx)("button",{className:"btn",children:"Download"})})]})},f=a(3),x=a(30);a(52);var O=function(e){var t=e.menuItem,a=e.id,n=(e.setId,function(e){return e?/<a\b[^>]*>/i.test(e)?e:e.replace(/(^|[^"'==])(https?:\/\/[^\s<)"']+)/g,(function(e,t,a){var s=a.replace(/[.,!?)]*$/,""),n=a.slice(s.length);return"".concat(t,'<a href="').concat(s,'" target="_blank" rel="noopener noreferrer">').concat(s,"</a>").concat(n)})):""}),i=Object(s.useState)(!0),o=Object(r.a)(i,2),c=o[0];return o[1],Object(m.jsx)("div",{className:"MenuItem",children:t.map((function(e){return Object(m.jsxs)("div",{className:"portfolio",children:[Object(m.jsx)("h5",{children:Object(m.jsx)("div",{className:"title",children:e.title})}),Object(m.jsxs)("div",{style:{maxHeight:"".concat(!c||-1!==a&&e.id!==a?"24vh":"400vh")},className:"project-content",children:[Object(m.jsx)("div",{className:"".concat(!c||-1!==a&&e.id!==a?"text-cover":"text-cover-plain")}),Object(m.jsxs)("div",{className:"flex-container",children:[Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[0]||"")}}),function(){if(e.images&&e.images[0])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[0],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[0]})]})}(),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[1]||"")}}),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[2]||"")}}),function(){if(e.images&&e.images[1])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[1],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[1]})]})}(),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[3]||"")}}),function(){if(e.images&&e.images[2])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[2],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[2]})]})}(),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[4]||"")}}),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[5]||"")}}),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[6]||"")}}),function(){if(e.images&&e.images[3])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[3],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[3]})]})}(),Object(m.jsx)("div",{className:"description",dangerouslySetInnerHTML:{__html:e.citations&&e.citations.length>0?'<h3>Links/Citations</h3><ul style="list-style-type: none; padding-left: 0;">'.concat(e.citations.map((function(e,t){return'<li style="margin-bottom: 5px;">['.concat(t+1,"] ").concat(n(e),"</li>")})).join(""),"</ul>"):""}})]})]})]},e.id)}))})},y=(a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p+"static/media/audiocraft1.7119c8f1.png"),v=(a.p,a.p+"static/media/dit.748b4c71.png"),w=a.p+"static/media/IMG_8809.033bb5d6.JPEG",N=a.p+"static/media/EntropyAudio (2).325a2056.png",k={synthlm1:"Given the impressive modeling capabilities of modern diffusion and flow models in the speech and music domains, it seems clear to me that there will be some form of natural language tool available for music composers in the future. Unfortunately, audio generation models aren't great for music composition in their current state. Some of these models may be able to generate full songs, but none are able to generate individual, high-quality samples well. To me this highlights both a large data bottleneck as well as a lack of focus on this specific task by the community.",synthlm2:"The open source model that first piqued my interest was <a href='https://musicgen.com/' target='_blank' rel='noopener noreferrer'>MusicGen from Meta AI</a> in summer 2023. This is an autoregressive transformer model that predicts time-steps in a compressed, discretized audio sequence conditioned on text. Although these models were a huge step up in open source at the time and implemented a cool approach to audio modeling, the limitations of autoregressive models became apparent when playing around with the model (consistency issues and slow generative speeds for long sequences). Autoregressive models aren't inherently bad, but they may not be the best fit for audio generation. Some other issues like artifact-free latent audio encoding and decoding were not completely solved at the time as well, lowering the quality of the model further. <a href='https://github.com/facebookresearch/encodec' target='_blank' rel='noopener noreferrer'>Encodec</a> (an audio autoencoder made by Meta) is solid, but not quite at the level needed for professional audio.",synthlm3:"By the end of the year (Dec 2023), I began experimenting and finetuning MusicGen. Through some testing I found that the autoencoder called <a href='https://github.com/descriptinc/descript-audio-codec' target='_blank' rel='noopener noreferrer'>Descript Audio Codec (DAC)</a> had superior audio quality, so I swapped out Encodec with DAC for these experiments. Unfortunately, experienced issues with model efficiency (MusicGen has 7B parameters), dataset breadth and quality, and sequence consistency. In June 2024, StabilityAI released <a href='https://huggingface.co/stabilityai/stable-audio-open-1.0' target='_blank' rel='noopener noreferrer'>Stable Audio Open (SAO)</a>. SAO is a 1B diffusion model that operates on a continuous latent space provided by a great encoder model called <a href='https://github.com/Harmonai-org/oobleck' target='_blank' rel='noopener noreferrer'>Oobleck</a>. SAO is superior to MusicGen in consistency, quality, simplicity, and efficiency. In my opinion, diffusion/flow models are the clear winner for audio generation.",synthlm4:"Unfortunately, the data bottleneck still remains. To fix this, I have been working on a dataset currently sitting at around 2TB. The largest portion of this data came from a program I made to automate the data gathering and labeling process. I used LLMs (Gemini 2.5 Flash (API) and Qwen3-30B (Local)) to generate captions given text metadata of an audio sample. Each datapoint in the dataset has metadata and conditioning fields prepared in a predefined schema and saved as a json file. This dataset design was inspired by the dataset classes in <a href='https://github.com/facebookresearch/audiocraft' target='_blank' rel='noopener noreferrer'>Meta's AudioCraft codebase</a>. The rest of the data is either open source or manually labeled. I am also hoping to create an audio classifier as well as a multimodal LLM trained to do labelling from raw audio as well. This is pending completion of the initial dataset to ensure I have lots of training data to play around with. Lastly, I use <a href='https://huggingface.co/docs/transformers/model_doc/clap' target='_blank' rel='noopener noreferrer'>CLAP score</a> to filter all open source and synthetic data to ensure that data is high quality. ",synthlm5:"On the engineering side, I created 4 core code packages: entropy_training, entropy_models, entropy_metrics, and entropy_data. The model package was initialized from the code in the <a href='https://github.com/Stability-AI/stable-audio-tools' target='_blank' rel='noopener noreferrer'>Stable Audio Tools</a> repo (which contained more than a few bugs, a good lesson in reviewing open source code and adding tests!) and contains code for the DiT, autoencoder, and conditioning modules. For text embeddings, I swapped out the original T5 used with SAO and added in Qwen3 Embedding, CLIP, and CLAP for diverse text features. The data package holds the dataset code as well as code for curating synthetic data and analyzing the dataset's distributions. The training package contains controller/orchestrator code for training and evaluating the model. Finally the metrics package contains metrics for evaluating audio. Some interesting metrics I have been using are the scores from <a href='https://ai.meta.com/research/publications/meta-audiobox-aesthetics-unified-automatic-quality-assessment-for-speech-music-and-sound/' target='_blank' rel='noopener noreferrer'>Meta's Audiobox Aesthetics</a>. This is a pretrained model that predicts scores for an audio's content enjoyment, content quality, production complexity, and production quality. These scores have actually been decent indicators of training progress and could potentially be used as reward models for post-training (this is an area I am really excited about experimenting with). I also use CLAP score and personal judgement for model evaluation.",synthlm6:"Initial/experimental training runs for the diffusion model were done on my local workstation with 1x5090. When I'm ready to scale I will probably move to a multi-gpu setup on RunPod. To speed up training, I used PyTorch's automatic mixed precision to convert the original float32 SAO weights down to bfloat16. Bfloat16 was preferred over float16 since it is just a simple precision/mantissa truncation on the float32 weights and no re-scaling is required. I also pre-encoded the audio latents since the CNN was a bottleneck during the training step. For training/experiment tracking I used wandb.",synthlm7:"For the Entropy Audio application, I created a <a href='https://github.com/EntropyAudio/entropy_frontend' target='_blank' rel='noopener noreferrer'>frontend with Angular</a> as well as a <a href='https://github.com/EntropyAudio/entropy_lambda' target='_blank' rel='noopener noreferrer'>small serverless backend</a> + <a href='https://github.com/EntropyAudio/entropy_inference' target='_blank' rel='noopener noreferrer'>model inference function</a>. For those backend pieces I used AWS Lambdas+S3+DDB and RunPod Serverless Endpoints respectively (see the design diagram above). My goal with the frontend was to make something that felt like a hybrid between digital synthesizer and LLM/chatbot UIs, and also integrate a natural data flywheel into the workflow. During generation, the user is presented with 4 options. The audio samples that the user selects to download are marked in a DDB table as 'preferred'. This way a synthetic preference dataset is automatically created as people use the app, helping to solve the data bottleneck mentioned earlier. RunPod/Lambda code can be found on my github. Core packages like entropy_training are private."},I=[{id:1,category:["2023"],link:"https://entropyaudio.io/",icon:d.a,descriptions:[k.synthlm1,k.synthlm2,k.synthlm3,k.synthlm4,k.synthlm5,k.synthlm6,k.synthlm7],title:"Generative, Transformer-Based Models For Music Composition",images:[y,v,N,w],imagecap:["1. MusicGen + Encodec Architecture","2. Diffusion Transformer (DiT) Paper","3. Generation Flow System Design","4. My Local Machine"],citations:["https://musicgen.com/","https://github.com/facebookresearch/audiocraft","https://huggingface.co/docs/transformers/model_doc/clap","https://huggingface.co/stabilityai/stable-audio-open-1.0","https://github.com/Stability-AI/stable-audio-tools","https://ai.meta.com/research/publications/meta-audiobox-aesthetics-unified-automatic-quality-assessment-for-speech-music-and-sound/","https://github.com/EntropyAudio/entropy_frontend","https://entropyaudio.io/","https://github.com/facebookresearch/encodec"]}];var _=[];I.map((function(e){return e.category.map((function(e){return _.push(e)}))}));var A=["All"].concat(Object(x.a)(_.filter((function(e,t,a){return a.indexOf(e)===t}))));var T=function(){var e=Object(s.useState)(A),t=Object(r.a)(e,2),a=(t[0],t[1],Object(s.useState)(I)),n=Object(r.a)(a,2),i=n[0],o=(n[1],Object(s.useState)(-1)),c=Object(r.a)(o,2),l=c[0],d=c[1];return Object(m.jsxs)("div",{className:"ProjectsPage",children:[Object(m.jsx)("div",{className:"title",children:Object(m.jsx)(p,{title:"Entropy Audio",progress:75})}),Object(m.jsx)("div",{className:"portfolio-menu",children:Object(m.jsx)(O,{menuItem:i,id:l,setId:d})})]})},S=a(31);var M=function(){var e=Object(S.a)("mqkwpwoa"),t=Object(r.a)(e,2),a=t[0];return t[1],a.succeeded&&(document.getElementsByClassName("success")[0].style.display="flex",document.getElementById("contact-form").reset()),Object(m.jsxs)("div",{className:"ContactPage",children:[Object(m.jsx)("div",{className:"contact-title",children:Object(m.jsx)(p,{title:"Contact",progress:100})}),Object(m.jsxs)("div",{className:"sections",children:[Object(m.jsxs)("div",{className:"context-info",children:["Phone: 607-339-1740",Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"Email: bej9@cornell.edu, bejordae@amazon.com",Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"Current Location: New York, NY"]}),Object(m.jsx)("div",{className:"map-sect",children:Object(m.jsx)("iframe",{src:"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d193572.132814464!2d-74.11808698000894!3d40.705825455231026!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89c24fa5d33f083b%3A0xc80b8f06e177fe62!2sNew%20York%2C%20NY!5e0!3m2!1sen!2sus!4v0000000000000!5m2!1sen!2sus",width:"600",height:"450",style:{border:0},allowFullScreen:"",loading:"lazy",referrerPolicy:"no-referrer-when-downgrade"})})]})]})},C=a(11);function L(){var e=Object(f.f)().pathname;return Object(s.useEffect)((function(){window.scrollTo(0,0)}),[e]),null}var E=function(){var e=Object(s.useState)(!1),t=Object(r.a)(e,2),a=t[0];return t[1],Object(m.jsxs)("div",{className:"App",children:[Object(m.jsx)("div",{className:"nav-",children:Object(m.jsxs)(f.c,{children:[Object(m.jsx)(f.a,{path:"/",exact:!0,children:Object(m.jsxs)(c.b,{className:"rightarrow",to:"/Projects",exact:!0,children:[Object(m.jsx)(C.b,{}),Object(m.jsx)(C.b,{className:"rightarrow2"})]})}),Object(m.jsxs)(f.a,{path:"/Projects",exact:!0,children:[Object(m.jsxs)(c.b,{className:"leftarrow",to:"/",exact:!0,children:[Object(m.jsx)(C.a,{}),Object(m.jsx)(C.a,{className:"leftarrow2"})]}),Object(m.jsxs)(c.b,{className:"rightarrow",to:"/Contact",exact:!0,children:[Object(m.jsx)(C.b,{}),Object(m.jsx)(C.b,{className:"rightarrow2"})]})]}),Object(m.jsx)(f.a,{path:"/Contact",exact:!0,children:Object(m.jsxs)(c.b,{className:"leftarrow",to:"/Projects",exact:!0,children:[Object(m.jsx)(C.a,{}),Object(m.jsx)(C.a,{className:"leftarrow2"})]})})]})}),Object(m.jsx)("div",{className:"sidebar ".concat(a?"nav-toggle":""),children:Object(m.jsx)(h,{})}),Object(m.jsx)("div",{className:"main-content",children:Object(m.jsx)("div",{className:"content",children:Object(m.jsxs)(f.c,{children:[Object(m.jsxs)(f.a,{path:"/",exact:!0,children:[Object(m.jsx)(L,{}),Object(m.jsx)(g,{})]}),Object(m.jsxs)(f.a,{path:"/Projects",exact:!0,children:[Object(m.jsx)(L,{}),Object(m.jsx)(T,{})]}),Object(m.jsxs)(f.a,{path:"/Contact",exact:!0,children:[Object(m.jsx)(L,{}),Object(m.jsx)(M,{})]})]})})})]})};o.a.render(Object(m.jsx)(n.a.StrictMode,{children:Object(m.jsx)(c.a,{children:Object(m.jsx)(E,{})})}),document.getElementById("root"))}},[[53,1,2]]]);
//# sourceMappingURL=main.94d4ff5e.chunk.js.map