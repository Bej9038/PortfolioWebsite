(this.webpackJsonpportfolio=this.webpackJsonpportfolio||[]).push([[0],{36:function(e,t,a){},37:function(e,t,a){},38:function(e,t,a){},39:function(e,t,a){},40:function(e,t,a){},41:function(e,t,a){},42:function(e,t,a){},43:function(e,t,a){},44:function(e,t,a){},53:function(e,t,a){"use strict";a.r(t);var s=a(1),n=a.n(s),i=a(24),o=a.n(i),c=a(10),r=(a(36),a(37),a(38),a(39),a(40),a(41),a(42),a(43),a(44),a.p,a(8)),l=a(12),d=a(7),m=a(0);var h=function(){return Object(m.jsx)("div",{className:"NavBar",children:Object(m.jsxs)("nav",{className:"nav",children:[Object(m.jsx)("header",{className:"hero",children:Object(m.jsxs)("div",{className:"icons",children:[Object(m.jsx)("a",{target:"_blank",href:"https://github.com/EntropyAudio",className:"icon-link",children:Object(m.jsx)(l.a,{icon:d.b,className:"icon"})}),Object(m.jsx)("a",{target:"_blank",href:"https://www.linkedin.com/in/ben-jordan-b745a0194/",className:"icon-link",children:Object(m.jsx)(l.a,{icon:d.c,className:"icon"})})]})}),Object(m.jsxs)("ul",{className:"nav-items",children:[Object(m.jsx)("lin",{className:"nav-item",children:Object(m.jsx)(r.b,{to:"/",exact:!0,activeClassName:"active",children:"Home"})}),Object(m.jsx)("lin",{className:"nav-item",children:Object(m.jsx)(r.b,{to:"/Projects",exact:!0,activeClassName:"active",children:"Projects"})}),Object(m.jsx)("lin",{className:"nav-item",children:Object(m.jsx)(r.b,{to:"/Contact",exact:!0,activeClassName:"active",children:"Contact"})})]}),Object(m.jsx)("div",{className:"footer footer",children:Object(m.jsx)("p",{children:"@2025 Ben Jordan"})})]})})};var j=function(e){var t=e.title,a=e.progress,s="title25 .9s ease forwards";return 50===a?s="title50 .9s ease forwards":75===a?s="title75 .9s ease forwards":100===a&&(s="title100 .9s ease forwards"),Object(m.jsxs)("div",{className:"Title",children:[Object(m.jsx)("div",{className:"page-bar",children:Object(m.jsx)("div",{className:"page-bar-progress",style:{animation:s}})}),Object(m.jsxs)("h3",{children:[t,Object(m.jsx)("span",{children:t})]})]})};var u=a.p+"static/media/Benjamin_Jordan_Resume_2025.d1411ac7.pdf",p=a.p+"static/media/IMG_0426.2b35c307.JPEG";var b=function(){return Object(m.jsx)("div",{className:"ImageSection",children:Object(m.jsxs)("div",{className:"image-content",children:[Object(m.jsx)("div",{className:"about-info",children:Object(m.jsxs)("p",{className:"about-text",children:[Object(m.jsx)("span",{children:"I am a software engineer at Amazon and a former graduate of Cornell & RIT CS."}),Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"In my current job, I work on large scale software systems with an emphasis on spark ETL jobs and ML ops. I have a background in ML from my masters as well as from my previous role and internship.",Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"Outside of work I like to spend my time on my project, Entropy Audio, which aims to redefine how we create sound for music composition. I also love trying new food, cool architecture, and keeping up with the latest trends in ML and ML systems."]})}),Object(m.jsx)("div",{className:"about-img-wrapper",children:Object(m.jsx)("img",{src:p,alt:"Benjamin Jordan headshot",className:"about-img"})})]})})};var f=function(){return Object(m.jsxs)("div",{className:"AboutPage",children:[Object(m.jsx)(j,{title:"Ben Jordan",progress:25}),Object(m.jsx)(b,{}),Object(m.jsx)(j,{title:"CV",progress:50}),Object(m.jsx)("a",{className:"download-link",style:{color:"var(--font-color2)"},href:u,download:!0,children:Object(m.jsx)("button",{className:"btn",children:"Download"})})]})},g=a(3),x=a(30);a(52);var O=function(e){var t=e.menuItem,a=e.id,n=(e.setId,function(e){return e?/<a\b[^>]*>/i.test(e)?e:e.replace(/(^|[^"'==])(https?:\/\/[^\s<)"']+)/g,(function(e,t,a){var s=a.replace(/[.,!?)]*$/,""),n=a.slice(s.length);return"".concat(t,'<a href="').concat(s,'" target="_blank" rel="noopener noreferrer">').concat(s,"</a>").concat(n)})):""}),i=Object(s.useState)(!0),o=Object(c.a)(i,2),r=o[0];return o[1],Object(m.jsx)("div",{className:"MenuItem",children:t.map((function(e){return Object(m.jsxs)("div",{className:"portfolio",children:[Object(m.jsx)("h5",{children:Object(m.jsx)("div",{className:"title",children:e.title})}),Object(m.jsxs)("div",{style:{maxHeight:"".concat(!r||-1!==a&&e.id!==a?"24vh":"400vh")},className:"project-content",children:[Object(m.jsx)("div",{className:"".concat(!r||-1!==a&&e.id!==a?"text-cover":"text-cover-plain")}),Object(m.jsxs)("div",{className:"flex-container",children:[Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[0]||"")}}),function(){if(e.images&&e.images[0])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[0],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[0]})]})}(),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[1]||"")}}),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[2]||"")}}),function(){if(e.images&&e.images[1])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[1],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[1]})]})}(),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[3]||"")}}),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[4]||"")}}),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[5]||"")}}),Object(m.jsx)("p",{className:"description",dangerouslySetInnerHTML:{__html:n(e.descriptions&&e.descriptions[6]||"")}}),function(){if(e.images&&e.images[2])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[2],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[2]})]})}(),function(){if(e.images&&e.images[3])return Object(m.jsxs)("div",{children:[Object(m.jsx)("img",{className:"img",src:e.images[3],alt:""}),Object(m.jsx)("div",{className:"img-caption",children:e.imagecap[3]})]})}()]})]})]},e.id)}))})},v=(a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p,a.p+"static/media/audiocraft1.7119c8f1.png"),y=(a.p,a.p+"static/media/dit.748b4c71.png"),w=a.p+"static/media/IMG_8809.033bb5d6.JPEG",N={synthlm1:"Given the impressive modeling capabilities of modern diffusion and flow-based models in the speech and music domains, it seems clear to me that there will be some form of natural langauge tool available for music composers in the future. Unfortunately, audio generation models are not suitable for music composition in their current state. Some of these models may be able to generate full songs, but none are able to generate high-quality, individual samples well. This fact highlights both a large data bottleneck as well as a lack of focus on this task by the community.",synthlm2:"The open source model that first peaked my interest was MusicGen from Meta AI in 2023. This is an autoregressive transformer model that aims to predict time-steps in a discretized, compressed audio sequence conditioned on text. Although these models were a huge step up in open source at the time and implement an interesting approach to audio modeling, the limitations of autoregressive models become apparent when playing around with the model (consistency issues and slow generative speeds for long sequences). This doesn't mean autoregressive models are inherently bad, I just think that maybe they are the best fit for the audio generation task. Some other issues like artifact-free latent audio encoding and decoding were not completely solved at this point, lowering the quality of the model further. Encodec (the model used in AudioGen) is solid, but not yet at the level needed for professional audio.",synthlm3:"By the end of the year (Dec 2023), I began experimenting with finetuning MusicGen with somewhat limited results. Through testing I determined that the autoencoder Descript Audio Codec (DAC) had superior audio quality so I swapped out Encodec with DAC for these experiments. Although DAC was an improvement on Encodec, I was still experiencing issues with model efficiency (AudioGen is a 7B model), dataset quality and breadth, and sequence consistency. In June 2024, StabilityAI released Stable Audio Open (SAO). SAO is a 1B diffusion model that operates on a continuous latent space provided by a newer encoder model called Oobleck. To me, SAO is superior to AudioGen in consistency, quality, simplicity, and efficiency. Diffusion/flow models are the clear winner for audio generation.",synthlm4:"However, the data bottleneck for this task still remains. To fix this, I have been working on a dataset that is currently sitting at around 2TB. The largest portion of this data came from a program I made to automate the data gathering and labeling process. I used LLMs Gemini 2.5 Flash (API) and Qwen3-30B (Local) to generate captions given metadata of an audio sample. Each datapoint has metadata and conditioning fields prepared in a predefined schema and saved as a json file. This dataset format was inspired by the dataset classes in <a href='https://github.com/facebookresearch/audiocraft' target='_blank' rel='noopener noreferrer'>Meta's AudioCraft codebase</a>. The rest of the data is either open source or manually labeled. When I get a chane, I am hoping to finetune a multimodal LLM to do labelling from raw audio as well. This is pending completion of the metadata-caption dataset, so I have lots of training data to play around with. I used <a href='https://huggingface.co/docs/transformers/model_doc/clap' target='_blank' rel='noopener noreferrer'>CLAP score</a> to filter all open source and synthetic data to ensure that data is high quality.",synthlm5:"On the engineering side, I created 4 main code packages: entropy_training, entropy_models, entropy_metrics, and entropy_data. The model package was initialized from the model code in the <a href='https://github.com/Stability-AI/stable-audio-tools' target='_blank' rel='noopener noreferrer'>Stable Audio Tools</a> repo (which surprisingly contained more bugs than I would have liked! A good lesson in testing your code and reviewing open source I guess) and contains code for the DiT, autoencoder, and conditioning module. For text embeddings, I swapped out the original T5 used with SAO and added in Qwen3 Embedding, CLIP, and CLAP for diverse text features. The data package holds the code for the audio dataset as well as code for curating synthetic data and analyzing the dataset distribution. The training package contains controller/orchestrator code for training and evaluating the model, and the metrics package has metrics for evaluation. Some interesting metrics I have been using are the model scores from <a href='https://ai.meta.com/research/publications/meta-audiobox-aesthetics-unified-automatic-quality-assessment-for-speech-music-and-sound/' target='_blank' rel='noopener noreferrer'>Meta's Audiobox Aesthetics</a>. This is a pretrained model that predicts scores for an audio's content enjoyment, content quality, production complexity, and production quality. These scores have actually been decent indicators of training progress and could potentially be useful for some experimental RL post-training. I also used CLAP score and personal judgement for evaluations.",synthlm6:"Initial/experimental training runs for the diffusion model were done on my local workstation with 1x5090. When I'm ready to scale I will probably move to a multi-gpu setup on RunPod (and use up some free startup credits I have there). To speed up training, I used PyTorch's automatic mixed precision to convert the original float32 SAO weights down to bfloat16. Bfloat16 was prefered over float16 as it is just a simple precision/mantissa truncation and no re-scaling is required. I also preencoded the audio latents ahead of time in my dataset as the CNN was a bottleneck during the training step. For training/experiment tracking I used wandb.",synthlm7:"I created a <a href='https://github.com/EntropyAudio/entropy_frontend' target='_blank' rel='noopener noreferrer'>frontend with Angular</a> as well as a small serverless backend + model inference function. For those backend pieces I used AWS Lambdas+S3+DDB and RunPod Serverless Endpoints respectively. My goal with the frontend was to make something that felt like a hybrid between digital synth and LLM UIs, and also to make a UI that allows for natural preference data selection (aka a data flywheel). RunPod/Lambda code can be found here: Note that the core packages like entropy_training are private."},I=[{id:1,category:["2023"],link:"https://entropyaudio.io/",icon:d.a,descriptions:[N.synthlm1,N.synthlm2,N.synthlm3,N.synthlm4,N.synthlm5,N.synthlm6,N.synthlm7],title:"Generative, Transformer-Based Models For Music Composition",images:[v,y,w],imagecap:["1. MusicGen + Encodec Architecture","2. Diffusion Transformer (DiT) Paper","3. My Local Machine"]}];var k=[];I.map((function(e){return e.category.map((function(e){return k.push(e)}))}));var A=["All"].concat(Object(x.a)(k.filter((function(e,t,a){return a.indexOf(e)===t}))));var T=function(){var e=Object(s.useState)(A),t=Object(c.a)(e,2),a=(t[0],t[1],Object(s.useState)(I)),n=Object(c.a)(a,2),i=n[0],o=(n[1],Object(s.useState)(-1)),r=Object(c.a)(o,2),l=r[0],d=r[1];return Object(m.jsxs)("div",{className:"ProjectsPage",children:[Object(m.jsx)("div",{className:"title",children:Object(m.jsx)(j,{title:"Entropy Audio",progress:75})}),Object(m.jsx)("div",{className:"portfolio-menu",children:Object(m.jsx)(O,{menuItem:i,id:l,setId:d})})]})},_=a(31);var S=function(){var e=Object(_.a)("mqkwpwoa"),t=Object(c.a)(e,2),a=t[0];return t[1],a.succeeded&&(document.getElementsByClassName("success")[0].style.display="flex",document.getElementById("contact-form").reset()),Object(m.jsxs)("div",{className:"ContactPage",children:[Object(m.jsx)("div",{className:"contact-title",children:Object(m.jsx)(j,{title:"Contact",progress:100})}),Object(m.jsxs)("div",{className:"sections",children:[Object(m.jsxs)("div",{className:"context-info",children:["Phone: 607-339-1740",Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"Email: bej9@cornell.edu, bejordae@amazon.com",Object(m.jsx)("br",{}),Object(m.jsx)("br",{}),"Current Location: New York, NY"]}),Object(m.jsx)("div",{className:"map-sect",children:Object(m.jsx)("iframe",{src:"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d193572.132814464!2d-74.11808698000894!3d40.705825455231026!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89c24fa5d33f083b%3A0xc80b8f06e177fe62!2sNew%20York%2C%20NY!5e0!3m2!1sen!2sus!4v0000000000000!5m2!1sen!2sus",width:"600",height:"450",style:{border:0},allowFullScreen:"",loading:"lazy",referrerPolicy:"no-referrer-when-downgrade"})})]})]})},M=a(11);function L(){var e=Object(g.f)().pathname;return Object(s.useEffect)((function(){window.scrollTo(0,0)}),[e]),null}var C=function(){var e=Object(s.useState)(!1),t=Object(c.a)(e,2),a=t[0];return t[1],Object(m.jsxs)("div",{className:"App",children:[Object(m.jsx)("div",{className:"nav-",children:Object(m.jsxs)(g.c,{children:[Object(m.jsx)(g.a,{path:"/",exact:!0,children:Object(m.jsxs)(r.b,{className:"rightarrow",to:"/Projects",exact:!0,children:[Object(m.jsx)(M.b,{}),Object(m.jsx)(M.b,{className:"rightarrow2"})]})}),Object(m.jsxs)(g.a,{path:"/Projects",exact:!0,children:[Object(m.jsxs)(r.b,{className:"leftarrow",to:"/",exact:!0,children:[Object(m.jsx)(M.a,{}),Object(m.jsx)(M.a,{className:"leftarrow2"})]}),Object(m.jsxs)(r.b,{className:"rightarrow",to:"/Contact",exact:!0,children:[Object(m.jsx)(M.b,{}),Object(m.jsx)(M.b,{className:"rightarrow2"})]})]}),Object(m.jsx)(g.a,{path:"/Contact",exact:!0,children:Object(m.jsxs)(r.b,{className:"leftarrow",to:"/Projects",exact:!0,children:[Object(m.jsx)(M.a,{}),Object(m.jsx)(M.a,{className:"leftarrow2"})]})})]})}),Object(m.jsx)("div",{className:"sidebar ".concat(a?"nav-toggle":""),children:Object(m.jsx)(h,{})}),Object(m.jsx)("div",{className:"main-content",children:Object(m.jsx)("div",{className:"content",children:Object(m.jsxs)(g.c,{children:[Object(m.jsxs)(g.a,{path:"/",exact:!0,children:[Object(m.jsx)(L,{}),Object(m.jsx)(f,{})]}),Object(m.jsxs)(g.a,{path:"/Projects",exact:!0,children:[Object(m.jsx)(L,{}),Object(m.jsx)(T,{})]}),Object(m.jsxs)(g.a,{path:"/Contact",exact:!0,children:[Object(m.jsx)(L,{}),Object(m.jsx)(S,{})]})]})})})]})};o.a.render(Object(m.jsx)(n.a.StrictMode,{children:Object(m.jsx)(r.a,{children:Object(m.jsx)(C,{})})}),document.getElementById("root"))}},[[53,1,2]]]);
//# sourceMappingURL=main.0c09fd87.chunk.js.map